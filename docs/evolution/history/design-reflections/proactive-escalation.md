# 主动求助机制：hardness-classify 与 AI 自我认知

> **本文档定位**：批判性审视 MaeDoc 的主动求助设计
> **所属系列**：[核心设计反思](./index.md)
> **相关文档**：[开放问题](../../questions/open-questions.md)

---

## 1. 设计理念

AGENTS.md §8 定义了「主动求助」规则：

> 当 AI 遇到无法高置信度回答的问题时，不应尝试给出答案，而是主动触发上报流程。

这是一个非常前瞻的设计——它承认了 AI 的能力边界，并试图建立一个「知道自己不知道」的机制。

---

## 2. 触发条件分析

当前定义了 6 个触发条件（H1-H6）：

| 条件 | 描述 |
|------|------|
| H1 | 高代价低置信：预判答案置信度为 LOW，且错误代价高 |
| H2 | 方案博弈僵局：存在多个同等合理的矛盾方案 |
| H3 | 专业知识缺口：需要特定领域专业知识 |
| H4 | 论证盲点自识：AI 意识到自己可能有系统性盲点 |
| H5 | 超出文档写作范畴：问题的实质是业务/战略决策 |
| H6 | 信息不完整无法补全：完成任务所需的关键信息缺失 |

这些条件覆盖了大部分需要人类介入的场景，但执行起来有挑战。

---

## 3. 实际效果

### 3.1 好的方面

- 确实避免了 AI 在不确定时「硬答」
- 上报包的内容越来越完整，外部 AI 能给出有针对性的建议

### 3.2 待改进的方面

**问题 1：触发阈值难以校准**

有时候 AI 过于保守，明明可以给出合理建议，却触发了主动求助。
有时候又过于自信，给出了一些似是而非的建议。

**问题 2：上报包的上下文边界**

上报包应该包含多少上下文？
- 太少：外部 AI 无法给出针对性建议
- 太多：用户隐私风险增加，且外部 AI 可能被无关信息干扰

目前的做法是「宁多勿少」，但这可能导致信息过载。

**问题 3：用户的时间成本**

每次主动求助，用户需要：
1. 审核上报内容
2. 去外部 AI 获取回答
3. 回到 MaeDoc 应用建议

如果频繁触发，用户的时间成本很高。

---

## 4. 已实施的改进

### 改进 1：渐进式求助（已实施）

MEDIUM 等级（4-6 分）不再静默地"建议部分求助"，而是主动使用 `question` 工具让用户选择：

1. 给出初步建议（带 `[MED]`/`[LOW]` 置信度标注），用户来判断是否采纳
2. 先回答 1-2 个关键追问，让建议更准确
3. 直接生成外部求助包

AGENTS.md §8.2 同步更新：区分了 MEDIUM（渐进式求助）和 HIGH（强制上报）的行为，并明确 MEDIUM 等级给出置信度标注的初步建议**不属于**绕过求助流程。

### 改进 2：置信度标注（部分实施）

`doc-content-fill` 的章节级信心等级和 AGENTS.md 的 `[HIGH]`/`[MED]`/`[LOW]` 标注规范已覆盖核心生成场景。`hardness-classify` MEDIUM 等级的初步建议也会带置信度标注。

待完成：在 `doc-outline-generate` 的大纲中添加整体置信度字段。

### 改进 3：上报包的智能摘要（未实施）

这一改进（生成摘要版 + 完整版供用户选择）仍在计划中，见[开放问题](../../questions/open-questions.md)。

---

## 5. 更深层的问题

主动求助机制的设计，触及了一个根本问题：**AI 如何知道自己不知道？**

这是一个开放的研究问题，没有标准答案。目前的做法是「启发式规则 + 上报模板」，但这显然不是最优解。

未来可能需要：
- 更精细的置信度评估模型
- 基于历史反馈的自我校准
- 与用户的长期信任建立

---

*本文档从 [design-reflections.md](./index.md) 拆分而来，最后更新：2026-02-21（已实施渐进式求助改进）*
