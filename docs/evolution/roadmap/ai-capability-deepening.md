# AI 能力深化：未来演进的核心方向

> **本文档定位**：探讨如何让 MaeDoc 的 AI 能力更强，聚焦「本地-远程协作」与「能力边界认知」
> **所属分组**：[方向与规划](./index.md)
> **相关文档**：[痛点与不足](../retrospect/pain-points.md)、[演进路线图](./evolution-roadmap.md)

---

## 1. 引言：为什么是「AI 能力深化」

在 [痛点与不足](../retrospect/pain-points.md) 中，我们分析了很多痛点。这些痛点的共同点是：**它们很大程度上受限于 AI 的能力边界**。

- 交互摩擦 → AI 理解意图不够准确，需要多轮确认
- 质量把控 → AI 生成的内容质量不够稳定
- 上下文断裂 → AI 的上下文窗口有限
- 迭代成本 → AI 对「修改意图」的理解不够精准

如果 AI 能力更强，这些痛点会自然缓解。

所以，「AI 能力深化」不是锦上添花，而是**解决根本问题**。

---

## 2. 什么是「AI 能力深化」

具体来说，我希望 MaeDoc 的 AI 在以下方面更强：

### 2.1 上下文理解

**当前状态**：AI 能理解单个文档，但对整个文档库的理解是割裂的。

**已实施改进**：`doc-outline-generate` 和 `doc-content-fill` 现在会在执行前读取 `docs/index.md` 及相关文档，提取现有术语定义和文档结构，在生成/填充时保持术语一致并通过链接引用避免内容重复。这是不需要向量数据库的轻量级跨文档上下文感知。

**期望状态**（仍未完全实现）：AI 能够：
- 理解文档之间的关联关系（部分实现）
- 在生成新内容时，自动引用已有内容（已实现）
- 检测并避免与已有内容矛盾（部分实现，依赖摘要读取）

### 2.2 推理深度

**当前状态**：AI 的推理相对浅层，容易给出「看起来对但实际有问题」的建议。

**期望状态**：AI 能够：
- 进行多步骤推理，识别潜在问题
- 提供论证过程，而非只有结论
- 主动指出假设和限制条件

### 2.3 自我改进

**当前状态**：AI 不会从用户的修改中学习。

**期望状态**：AI 能够：
- 记住用户的偏好和修改习惯
- 从历史交互中提取模式
- 逐步提高对特定用户/项目的适配度

### 2.4 能力边界认知

**当前状态**：AI 对自己的能力边界判断不够准确。

**期望状态**：AI 能够：
- 准确评估自己的置信度
- 在不确定时主动说明
- 知道什么时候该求助

---

## 3. 本地与远程的协同进化

虽然本地模型是核心，但远程模型（Claude、GPT-4 等）仍然有价值。关键是如何设计协作模式。



### 3.1 当前协作模式的问题

当前的 `.docforge` 中继协议是「手动触发 + 手动应用」。这种模式：
- 用户体验差（复制粘贴繁琐）
- 无法实时协作
- 缺乏上下文同步

### 3.2 理想的协作模式

**模式 1：自动升级**

当本地模型判断自己无法处理时，自动将请求升级到远程模型。用户可以选择：
- 预先授权：允许自动升级
- 逐次确认：每次升级前询问

**模式 2：混合处理**

本地模型负责大部分工作，远程模型负责「关键时刻」：
- 本地：大纲生成、内容填充
- 远程：复杂推理、专业审阅

**模式 3：无缝切换**

用户无感知地在本地和远程之间切换。AI 根据任务复杂度自动选择最合适的模型。

### 3.3 协作模式的技术挑战

**挑战 1：API Key 管理**

如果支持自动调用远程 API，需要安全地存储和管理 API Key。

**可能的解决方案**：
- 使用系统 Keychain
- 环境变量注入
- 每次使用时手动输入

**挑战 2：成本控制**

远程 API 调用有成本。需要机制防止意外的大量调用。

**可能的解决方案**：
- 设置调用频率限制
- 设置成本上限
- 提供使用量统计

**挑战 3：隐私保护**

自动调用远程 API 意味着内容会发送到第三方服务。

**可能的解决方案**：
- 明确的隐私策略
- 敏感内容过滤
- 用户可控的发送范围

### 3.4 推荐的演进路径

1. **第一阶段**：保持文件中继，优化交互
   - 改进 `/escalate` 的体验
   - 优化上报包的格式
   - 改进 `/ingest-remote` 的建议分类

2. **第二阶段**：引入可选的 API 集成
   - 支持配置 API Key
   - 提供手动触发 API 调用的命令
   - 用户可以选择使用或不使用

3. **第三阶段**：自动升级机制
   - AI 判断是否需要升级
   - 用户预先授权或逐次确认
   - 成本和隐私控制

---

## 4. 能力边界认知的提升

「知道自己不知道」是 AI 安全和可靠性的关键。

### 4.1 当前的做法

通过 `hardness-classify` Skill 和 AGENTS.md §8 的主动求助规则，MaeDoc 已经有了能力边界认知机制。

**已实施改进**：
- MEDIUM 等级（4-6 分）现在触发「渐进式求助」：使用 `question` 工具让用户选择（初步建议+置信标注 / 追问补充信息 / 生成外部求助包），而非原先静默的"建议部分求助"
- AGENTS.md §8.2 已明确区分 MEDIUM 与 HIGH 等级的行为，并澄清 MEDIUM 等级给出置信度标注的初步建议**不属于**绕过求助流程

**仍存在的挑战**：
- 触发阈值的精准度仍依赖启发式规则，而非量化指标
- 用户采纳/修改 AI 建议的反馈信号尚未被系统记录，无法用于自动校准

### 4.2 提升方向

#### 方向 1：置信度量化

**现状**：`doc-content-fill` 已实现章节级信心等级标注（🟢高/🟡中/🔴低），AGENTS.md §7 已定义 `[HIGH]`/`[MED]`/`[LOW]` 标注规范。`hardness-classify` 的 MEDIUM 等级现在也会在初步回答中显式标注置信度。

**还需做的**：将置信度标注扩展到 `doc-outline-generate` 的大纲整体置信度字段，以及 `doc-evaluate` 的各维度评估中。

#### 方向 2：不确定性显式化

**现状**：AGENTS.md §3.2「未指定即标注」已要求使用 `[未指定]` 标记缺失信息；`doc-content-fill` 已要求用 `[假设: ...]` 和 `[待确认: ...]` 标注不确定内容。`hardness-classify` 的 MEDIUM 渐进式求助也会在建议中标注 `[MED]`/`[LOW]`。

**还需做的**：在 `doc-outline-generate` 的大纲中区分高/低置信度的章节，让用户在大纲确认阶段就能感知风险点。

#### 方向 3：基于历史的校准

记录 AI 的判断和实际结果，用于校准置信度：

- AI 判断 [HIGH]，用户采纳 → 校准为更高置信
- AI 判断 [HIGH]，用户大幅修改 → 校准为更低置信

这需要一个「反馈收集」机制。

### 4.3 实施建议

1. **短期（已完成）**：渐进式求助（MEDIUM 级别）+ 基础文档库上下文感知
2. **中期（待完成）**：在大纲中添加章节级置信度字段；扩展不确定性标注到 `doc-evaluate`
3. **长期（待完成）**：引入基于历史用户反馈的置信度校准系统（需要持久化存储支持）

---

## 5. 小结：AI 能力深化的路线图

| 阶段 | 重点 | 状态 |
|------|------|------|
| 已完成 | 渐进式求助（MEDIUM）、基础跨文档上下文感知 | ✅ |
| 短期（1-2 周） | 提示工程优化（大纲/内容 Skill 提示词）、置信度扩展到大纲层 | 计划中 |
| 中期（1-2 月） | RAG 集成（向量索引、跨文档检索） | 探索中 |
| 长期（3-6 月） | 自动升级机制、历史驱动置信度校准 | 未开始 |

AI 能力深化是一个持续的过程，不可能一蹴而就。关键是：
1. 每次改进都要有明确的度量
2. 保持向后兼容
3. 用户可以选择是否启用新能力

下一章，我们将把这些想法转化为具体的版本规划。

---

*本章最后更新：2026-02-22（已删除「3. 本地模型的强化路径」章节）*
