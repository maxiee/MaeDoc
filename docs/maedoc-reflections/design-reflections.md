# 核心设计反思

> **本文档定位**：对 MaeDoc 四个关键设计进行批判性审视
> **阅读建议**：本章是整份文档的「深度思考」部分，建议花时间细读

---

## 引言：为什么要反思

设计决策在当时往往是「最优解」，但随着项目演进和认知深化，当初的权衡可能不再适用。

本章不想做「事后诸葛亮」式的批评，而是想回答一个问题：**如果我们重新设计，会做出什么不同的选择？**

这四个设计是 MaeDoc 的核心：
1. Skills 架构
2. /create 自由化
3. 远程桥接机制
4. 主动求助机制

---

## 1. Skills 架构：8 个能力的设计哲学

### 1.1 当初的设计

把写作能力拆分成 8 个独立的 Skills，每个负责一件事：

```
用户意图 → /create 命令 → Skills 编排 → 输出文档
```

理论上的好处：
- **可组合**：不同 Skills 可以灵活组合
- **可复用**：单个 Skill 可以被不同命令调用
- **可测试**：每个 Skill 有明确的输入输出

### 1.2 实际的问题

**问题 1：用户不直接使用 Skills**

我几乎从未直接调用某个 Skill。通常是通过 `/create` 间接使用，或者直接跟 AI 说「帮我审阅这篇文档」。

这意味着 Skills 层的价值被稀释了——它变成了命令层的内部实现细节，而非用户可见的能力。

**问题 2：Skills 之间的边界模糊**

举个例子：
- `doc-review`：多维度审阅
- `doc-quality-score`：量化评分
- `doc-structure-audit`：结构检查

这三个 Skill 有大量重叠。`doc-review` 本身就应该包含质量评分和结构检查。把它们拆开，反而增加了理解成本。

**问题 3：Skills 的粒度不一致**

- `doc-translate` 是一个「原子操作」，职责非常清晰
- `doc-content-fill` 是一个「复杂流程」，内部需要处理很多子问题

把这两个放在同一层级，感觉不太对。

### 1.3 如果重新设计

我可能会这样重新划分：

**Level 1：核心能力（面向 AI 内部使用）**
- `generate-outline`：生成大纲
- `fill-content`：填充内容
- `format-document`：格式化

**Level 2：分析能力（面向用户可选使用）**
- `review-document`：综合审阅（包含质量评分、结构检查等）
- `analyze-gap`：差距分析（文档 vs 理想状态）

**Level 3：操作能力**
- `translate`：翻译
- `iterate`：迭代修改

这样的划分逻辑是：**按使用场景分层，而非按功能拆分**。

### 1.4 留下的开放问题

- Skills 是否应该对用户可见？还是应该完全隐藏在命令层后面？
- 如果 Skills 对用户可见，如何让用户理解它们的用途？
- Skills 的版本如何管理？升级后如何保证兼容性？

---

## 2. /create 自由化：从模板到直觉

### 2.1 从模板系统到自由创作

v0022 删除模板系统，是一个重要的转折点。

**删除前**：
```
用户描述 → 匹配文档类型 → 套用模板章节 → 填充内容
```

**删除后**：
```
用户描述 → AI 判断结构 → 生成大纲 → 确认 → 填充内容
```

初衷是好的：让 AI 不被预设框架限制，根据实际内容生成最合适的结构。

### 2.2 自由化的代价

**代价 1：可预测性降低**

使用模板时，我知道 tech-design 一定有「背景与目标」「架构设计」「风险与缓解」这些章节。

删除模板后，每次生成大纲都是「惊喜」。有时候惊喜是好的，有时候是坏的。

**代价 2：交互成本增加**

因为 AI 生成的结构不一定符合预期，我经常需要「修改大纲」。这增加了交互轮次。

**代价 3：AI 的「直觉」不可靠**

AI 判断「这篇内容适合什么结构」的能力，取决于它对领域知识的掌握。对于常见场景（技术设计、博客），它做得还可以。但对于更小众的场景，它的判断可能偏离预期。

### 2.3 如果重新设计

我认为需要某种「半结构化」的中间形态：

**方案 A：参考结构库（非强制）**

维护一组「参考结构」，AI 在生成大纲时可以参考，但不是必须遵守。用户可以：
- 显式指定：`/create --style=tech-design ...`
- 让 AI 自主选择：`/create ...`（AI 参考结构库但不强制）

**方案 B：结构偏好记忆**

记住用户的历史偏好。如果用户多次修改大纲把「安全考量」加回去，下次自动包含。

**方案 C：渐进式结构确认**

先生成一个粗粒度大纲（只列出主要章节），确认后再细化子章节。这样减少「全盘推翻」的情况。

### 2.4 当前的权衡

目前我倾向于**方案 B + 方案 C 的组合**：
- 记住用户的结构偏好
- 采用渐进式大纲生成，减少大规模返工

但实现这个需要更复杂的状态管理，是一个不小的工程。

---

## 3. 远程桥接：.docforge 中继协议

### 3.1 设计初衷

当本地模型能力不足时，通过文件中继桥接到更强大的外部 AI。

选择文件中继而非 API 调用，原因是：
- **隐私**：用户可以先审核要发送的内容
- **通用性**：支持任何外部 AI，不绑定特定平台
- **简单**：不需要管理 API Key

### 3.2 实际体验

说实话，我很少手动使用 `/escalate`。

更常见的情况是：在对话过程中，AI 自己通过 `hardness-classify` 触发「主动求助」，这时我才被动地使用远程桥接。

**问题 1：手动复制粘贴繁琐**

`outbox/*.md → 复制 → 打开 Claude.ai → 粘贴 → 等回答 → 复制 → 保存到 inbox/*.md → /ingest-remote`

这个流程至少需要 5 个手动步骤，摩擦很大。

**问题 2：上下文同步成本**

外部 AI 的回答只是「建议」，需要手动决定采纳哪些、跳过哪些。如果外部 AI 的建议与本地上下文有冲突，解决起来很麻烦。

**问题 3：反馈循环缺失**

外部 AI 的回答是「一次性」的。如果我有追问，需要重新走一遍流程。

### 3.3 如果重新设计

**方案 A：自动 API 调用（可选）**

提供配置选项，允许用户配置 API Key。当触发远程增强时，自动调用 API 获取回答，跳过手动复制粘贴。

代价是：
- 需要管理 API Key
- 隐私风险（自动发送内容）

**方案 B：浏览器自动化**

使用 Playwright 等工具，自动打开外部 AI 网页，自动填入问题，自动获取回答。

代价是：
- 依赖浏览器环境
- 外部 AI 网页可能变化

**方案 C：保持现状，优化交互**

不改变文件中继的本质，但优化交互：
- 生成更紧凑的 outbox 文件
- 提供「一键导入」功能（监控 inbox 目录变化，自动提示）
- 在 /ingest-remote 时提供更智能的建议分类

### 3.4 我的倾向

短期内，我倾向于**方案 C**：保持文件中继，优化交互。

长期来看，如果本地模型能力持续提升，远程桥接的使用频率会下降，这个问题可能自然消失。

---

## 4. 主动求助机制：hardness-classify 与 AI 自我认知

### 4.1 设计理念

AGENTS.md §8 定义了「主动求助」规则：

> 当 AI 遇到无法高置信度回答的问题时，不应尝试给出答案，而是主动触发上报流程。

这是一个非常前瞻的设计——它承认了 AI 的能力边界，并试图建立一个「知道自己不知道」的机制。

### 4.2 触发条件分析

当前定义了 6 个触发条件（H1-H6）：

| 条件 | 描述 |
|------|------|
| H1 | 高代价低置信：预判答案置信度为 LOW，且错误代价高 |
| H2 | 方案博弈僵局：存在多个同等合理的矛盾方案 |
| H3 | 专业知识缺口：需要特定领域专业知识 |
| H4 | 论证盲点自识：AI 意识到自己可能有系统性盲点 |
| H5 | 超出文档写作范畴：问题的实质是业务/战略决策 |
| H6 | 信息不完整无法补全：完成任务所需的关键信息缺失 |

这些条件覆盖了大部分需要人类介入的场景，但执行起来有挑战。

### 4.3 实际效果

**好的方面**：
- 确实避免了 AI 在不确定时「硬答」
- 上报包的内容越来越完整，外部 AI 能给出有针对性的建议

**待改进的方面**：

**问题 1：触发阈值难以校准**

有时候 AI 过于保守，明明可以给出合理建议，却触发了主动求助。
有时候又过于自信，给出了一些似是而非的建议。

**问题 2：上报包的上下文边界**

上报包应该包含多少上下文？
- 太少：外部 AI 无法给出针对性建议
- 太多：用户隐私风险增加，且外部 AI 可能被无关信息干扰

目前的做法是「宁多勿少」，但这可能导致信息过载。

**问题 3：用户的时间成本**

每次主动求助，用户需要：
1. 审核上报内容
2. 去外部 AI 获取回答
3. 回到 MaeDoc 应用建议

如果频繁触发，用户的时间成本很高。

### 4.4 如果重新设计

**改进 1：渐进式求助**

不是直接跳到「生成上报包」，而是先询问用户：

```
我在分析这个问题时感到不确定。
我可以：
1. 给出一个带 [LOW] 置信度的初步建议
2. 向你提问以获取更多信息
3. 打包上下文，让你咨询外部 AI
```

这样用户有选择权，可以根据实际情况决定。

**改进 2：置信度校准**

在 AI 的输出中显式标注置信度，让用户对 AI 的判断有感知。例如：

```
## 架构建议 [置信度: HIGH]

## 性能优化方案 [置信度: MED]

## 安全威胁模型 [置信度: LOW]
```

这样可以建立用户对 AI 能力边界的认知。

**改进 3：上报包的智能摘要**

生成上报包时，自动生成一份「摘要版」，包含最关键的信息。用户可以选择发送摘要版或完整版。

### 4.5 更深层的问题

主动求助机制的设计，触及了一个根本问题：**AI 如何知道自己不知道？**

这是一个开放的研究问题，没有标准答案。目前的做法是「启发式规则 + 上报模板」，但这显然不是最优解。

未来可能需要：
- 更精细的置信度评估模型
- 基于历史反馈的自我校准
- 与用户的长期信任建立

---

## 5. 总结：设计的本质是权衡

回顾这四个设计，我发现一个共同点：**每个设计都是在做权衡**。

| 设计 | 获得的 | 失去的 |
|------|--------|--------|
| Skills 架构 | 可组合、可复用 | 用户理解成本、边界模糊 |
| /create 自由化 | 灵活性、适应性 | 可预测性、交互效率 |
| 远程桥接 | 隐私、通用性 | 用户体验、效率 |
| 主动求助 | 避免错误建议 | 用户时间成本 |

这些权衡在当时可能是对的，但随着项目演进和需求变化，可能需要重新调整。

**设计不是一次性决策，而是持续演化的过程。**

下一章，我们将看看当前设计的具体痛点，以及可能的改进方向。

---

*本章最后更新：2026-02-21*
